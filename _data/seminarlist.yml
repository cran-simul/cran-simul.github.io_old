-  speaker: Radu Ranta
   institution: CRAN, Université de Lorraine
   date:  June, 24th 2021, 10h-11h
   title: Low-Rank Inverse Problems in Brain Signal Processing
   abstract: "The presentation will start by introducing the basics principles of biophysics allowing to model the electrophysiological brain measurements (EEG / SEEG / micro-electrodes), and more precisely the relations between the neural current sources and the electrodes. Once the signal model defined, I will briefly present some of the classical methods for solving the inverse problem of brain sources estimation (localization and activity), and I will focus next on our work on sparse approximations and low-rank (exact and approximate) source estimates."
   active: 0
   
-  speaker: Gaëtan Frusque
   institution: ETH Zürich
   date:  April, 16th 2021, 10h-11h
   title: Inférence et décomposition de graphes dynamiques en neurosciences
   abstract: "Dynamic graphs make it possible to understand the evolution of complex systems which evolve through time. In this thesis, we look at their applications to understand one of the most common neurological disorder in the world, affecting around 1% of the population: epilepsy. A complete and objective characterization of the patient-specific dynamic graph describing this pathology is crucial for optimal surgical treatment. First, we propose to modify a measure of functional connectivity, the Phase-Locking-Value, in order to infer robust dynamic graph from the neurophysiological signal recorded during an epileptic seizure. Constrained matrix decomposition method is applied to extract the principal features from the dynamic graph describing the pathology. Finally, a clinical study is performed to compare the obtained features from the visual interpretation of a clinician specialized in neurophysiological signal interpretation."
   active: 0

- speaker: Titouan Parcollet
  institution: Université d’Avignon
  date:  March, 24th 2021, 14h-15h
  title: Should we use quaternion neural networks? Recent advances and limitations.
  abstract: Real-world data used to train modern artificial neural networks reflect the complexity of the environment that we are evolving in. As a consequence, they are neither flat, nor decorrelated nor one dimensional. Instead, scientists have to deal with composed and multidimensional entities, that are characterized by multiple related components, such as color channels describing a single color pixel of an image, or the 3D coordinates of a point denoting the position of a robot. Surprisingly, recent advances on deep learning are mainly focused on developing novel architectures to extract even more relevant and robust high level representations from the input features, while the latter are still poorly considered at a lower and basic level, by being processed with one dimensional real-valued neural models. Neural networks based on complex and quaternion numbers have been used sparsely for many decades. Nonetheless, due to new statements and proofs about the benefits of these models over real-valued ones on many real-world tasks, quaternion based neural networks have been increasingly employed, and novel quaternion based architectures have been proposed. This talk will detail quaternion neural networks architectures for artificial intelligence related tasks, such as image processing, or speech recognition, by introducing first the basics of quaternion numbers, and then describing recent advances on quaternion neural networks with the quaternion convolutional (Interspeech 2018, ICASSP 2019) and recurrent neural networks (ICLR 2019, Interspeech 2020). This presentation will also show their benefits in terms of performances obtained in different tasks, as well as in terms of neural parameters required for learning. Finally, the talk will outline important future research directions to turn quaternion neural networks into a mandatory alternative to real-valued models for real-world tasks. 
  active: 0

 





